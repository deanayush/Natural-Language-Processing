{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "PpcoqA-3l2g9",
   "metadata": {
    "id": "PpcoqA-3l2g9"
   },
   "source": [
    "## Installing Sentence Transsformer and other models/frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0tw0xe31hT59",
   "metadata": {
    "id": "0tw0xe31hT59"
   },
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "\n",
    "# Kindly add all your installations and versions if any in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WWWTQtXDl-t8",
   "metadata": {
    "id": "WWWTQtXDl-t8"
   },
   "source": [
    "## Importing necessary libraries. \n",
    "In the final version all imports should be stricly enlisted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e951f9be",
   "metadata": {
    "id": "e951f9be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 23:18:42.066649: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-04 23:18:42.295584: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-04 23:18:42.440029: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-04 23:18:43.540565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 23:18:43.540611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 23:18:43.540616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-01-04 23:18:44.652661: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-04 23:18:44.652679: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-04 23:18:44.652692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ayush-Mi-NoteBook-Ultra): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fasttext.util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5Ch9I58mMGe",
   "metadata": {
    "id": "H5Ch9I58mMGe"
   },
   "source": [
    "## Load dataset: 7 marks\n",
    "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
    "\n",
    "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
    "\n",
    "3 Create 3 dataframes one each for train, dev and val and print their final shapes. **1.5 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2TMR0Z0DlfFf",
   "metadata": {
    "id": "2TMR0Z0DlfFf"
   },
   "outputs": [],
   "source": [
    "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
    "    path = \"/home/ayush/NLP_Assignments/A3_Ayush_Jahnvi/stsbenchmark/\" +\"sts-\"+ dataset_type + \".csv\"\n",
    "    return pd.read_csv(path, sep='\\\\t', on_bad_lines='skip',names=columns)\n",
    "\n",
    "\n",
    "df_train = read_sts_csv(\"train\")\n",
    "df_test = read_sts_csv(\"test\")\n",
    "df_dev = read_sts_csv(\"dev\")\n",
    "\n",
    "# df_<dataset_type> = read_sts_csv(dataset_type) \n",
    "# create the train, dev and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303c432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5552, 7)\n",
      "(1478, 7)\n",
      "(1095, 7)\n"
     ]
    }
   ],
   "source": [
    "#shapes of the 3 datasets\n",
    "print(df_train.shape)\n",
    "print(df_dev.shape)\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66033596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>sent_a</th>\n",
       "      <th>sent_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>4</td>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>5</td>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>6</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>9</td>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source    type      year  id  score  \\\n",
       "0  main-captions  MSRvid  2012test   1   5.00   \n",
       "1  main-captions  MSRvid  2012test   4   3.80   \n",
       "2  main-captions  MSRvid  2012test   5   3.80   \n",
       "3  main-captions  MSRvid  2012test   6   2.60   \n",
       "4  main-captions  MSRvid  2012test   9   4.25   \n",
       "\n",
       "                                          sent_a  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "\n",
       "                                              sent_b  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  \n",
       "3                         Two men are playing chess.  \n",
       "4                 A man seated is playing the cello.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gO2ZkIwDmo4s",
   "metadata": {
    "id": "gO2ZkIwDmo4s"
   },
   "source": [
    "## Hyperparameters: 5 Marks\n",
    "Update this cell with you choosen parameters except, NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4QurhOG7E0Z-",
   "metadata": {
    "id": "4QurhOG7E0Z-"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cc.en.300.bin cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m NON_CONEXTUAL_MODEL_TYPE \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc.en.300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m CONEXTUAL_MODEL_TYPE \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fasttext/FastText.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fasttext/FastText.py:94\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mfasttext()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cc.en.300.bin cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "\n",
    "NON_CONEXTUAL_MODEL_TYPE = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "CONEXTUAL_MODEL_TYPE = models.Transformer('distilroberta-base')\n",
    "\n",
    "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')# USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
    "# INPUT_PATH = <INPUT_FOLDER_PATH>\n",
    "BATCH_SIZE = 16\n",
    "OUT_DIM_DENSE = 256\n",
    "NUM_EPOCHS = 3 ## THIS IS FIXED DO NOT CHANGE\n",
    "\n",
    "# You are free to add your own hyperparameters as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KgpbPlH9nXDy",
   "metadata": {
    "id": "KgpbPlH9nXDy"
   },
   "source": [
    "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
    "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
    "\n",
    "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
    "\n",
    "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
    "\n",
    "3 Print the correlation scores on the dev and dev set predictions using trained `model1`. **1.5 mark**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Hr7teQO9nfRR",
   "metadata": {
    "id": "Hr7teQO9nfRR"
   },
   "outputs": [],
   "source": [
    "def get_feature_model1(data_frame):\n",
    "    \"\"\"\n",
    "    Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
    "    Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
    "    \"\"\"\n",
    "    df_sent = data_frame[[\"sent_a\", \"sent_b\"]]\n",
    "    list_A, list_B = [], []\n",
    "    for row in range(len(df_sent)):\n",
    "    list_A.append(NON_CONEXTUAL_MODEL_TYPE.get_sentence_vector(df_sent.loc[row, \"sent_a\"]))\n",
    "    list_B.append(NON_CONEXTUAL_MODEL_TYPE.get_sentence_vector(df_sent.loc[row, \"sent_b\"]))\n",
    "\n",
    "    return (np.array(list_A), np.array(list_B))  \n",
    "\n",
    " \n",
    "## Non contextual language model\n",
    "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
    "X_train_tup, Ytrain = np.array(list(zip(feature_1_train, feature_2_train))), np.array(df_train[\"score\"])\n",
    "X_train = np.reshape(X_train_tup, (len(df_train), 600))\n",
    "\n",
    "#word embedding of each sentence is a 300 sized vector, final df of 2 sentences containing 600 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59fc0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(epsilon=0.2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(epsilon=0.2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(epsilon=0.2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svr', SVR(epsilon=0.2))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate a regression model and train it.\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "clf = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "clf.fit(X_train, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cd4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Score on dev: 0.28387182507452213\n"
     ]
    }
   ],
   "source": [
    "# # Print spearman correlation on the predicted output of the dev set.\n",
    "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
    "X_dev_tup, Ydev = np.array(list(zip(feature_1_dev, feature_2_dev))), np.array(df_dev[\"score\"])\n",
    "X_dev = np.reshape(X_dev_tup, (len(df_dev), 600))\n",
    "Y_pred_dev = clf.predict(X_dev)\n",
    "spear_dev = stats.spearmanr(Ydev, Y_pred_dev)\n",
    "print(\"Spearman Score on dev: \" + str(spear_dev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e1424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Score on test: 0.47763114660927947\n"
     ]
    }
   ],
   "source": [
    "# # Print spearman correlation on the predicted output of the test sets.\n",
    "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
    "X_test_tup, Ytest = np.array(list(zip(feature_1_test, feature_2_test))), np.array(df_test[\"score\"])\n",
    "X_test = np.reshape(X_test_tup, (len(df_test), 600))\n",
    "Y_pred_test = clf.predict(X_test)\n",
    "spear_test = stats.spearmanr(Ytest, Y_pred_test)\n",
    "print(\"Spearman Score on test: \" + str(spear_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DBzjbQ-grL8H",
   "metadata": {
    "id": "DBzjbQ-grL8H"
   },
   "source": [
    "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
    "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
    "\n",
    "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
    "\n",
    "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
    "\n",
    "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
    "\n",
    "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "GlTVNjv0sNP0",
   "metadata": {
    "id": "GlTVNjv0sNP0"
   },
   "outputs": [],
   "source": [
    "def get_feature_model2(data_frame: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
    "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
    "  \"\"\"\n",
    "    df_sent = data_frame[[\"sent_a\", \"sent_b\"]]\n",
    "    list_A, list_B = [], []\n",
    "    sent_list_A = data_frame[\"sent_a\"].to_list()\n",
    "    sent_list_B = data_frame[\"sent_b\"].to_list()\n",
    "\n",
    "    embeddings1 = HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL.encode(sent_list_A, convert_to_tensor=True)\n",
    "    embeddings2 = HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL.encode(sent_list_B, convert_to_tensor=True)\n",
    "\n",
    "    return (np.array(embeddings1), np.array(embeddings2))\n",
    "\n",
    "\n",
    "# non_cont_model2\n",
    "feature_1_train_cont, feature_2_train_cont = get_feature_model2(df_train)\n",
    "X_train_cont_tup, Y_train_cont = np.array(list(zip(feature_1_train_cont, feature_2_train_cont))), np.array(df_train[\"score\"])\n",
    "# print(np.array(feature_1_train_cont).shape)\n",
    "# print(feature_1_train_cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d316520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(epsilon=0.2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(epsilon=0.2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(epsilon=0.2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svr', SVR(epsilon=0.2))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Support vector regressor model created\n",
    "# output of sentence transformer is a 384 dimension vector for 1 sentence, 768 for both \n",
    "X_train_cont = np.reshape(X_train_cont_tup, (len(df_train), (384*2)))\n",
    "clf_cont = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "clf.fit(X_train_cont, Y_train_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1c5ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Score on dev: 0.4694053543121098\n"
     ]
    }
   ],
   "source": [
    "# print spearman score on dev set\n",
    "feature_1_dev_cont, feature_2_dev_cont = get_feature_model2(df_dev)\n",
    "X_dev_tup_cont, Ydev = np.array(list(zip(feature_1_dev_cont, feature_2_dev_cont))), np.array(df_dev[\"score\"])\n",
    "X_dev_cont = np.reshape(X_dev_tup_cont, (len(df_dev), 768))\n",
    "Y_pred_dev_cont = clf.predict(X_dev_cont)\n",
    "spear_dev = stats.spearmanr(Ydev, Y_pred_dev_cont)\n",
    "print(\"Spearman Score on dev: \" + str(spear_dev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Score on test: 0.5767784080921666\n"
     ]
    }
   ],
   "source": [
    "# print spearman score on test set\n",
    "feature_1_test_cont, feature_2_test_cont = get_feature_model2(df_test)\n",
    "X_test_tup_cont, Ytest = np.array(list(zip(feature_1_test_cont, feature_2_test_cont))), np.array(df_test[\"score\"])\n",
    "X_test_cont = np.reshape(X_test_tup_cont, (len(df_test), 768))\n",
    "Y_pred_test_cont = clf.predict(X_test_cont)\n",
    "spear_test = stats.spearmanr(Ytest, Y_pred_test_cont)\n",
    "print(\"Spearman Score on test: \" + str(spear_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VImljTWps_GR",
   "metadata": {
    "id": "VImljTWps_GR"
   },
   "source": [
    "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
    "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
    "\n",
    "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
    "\n",
    "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
    "\n",
    "6 Initiate the `loss`. **0.5 marks**\n",
    "\n",
    "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
    "\n",
    "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
    "\n",
    "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
    "\n",
    "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0kb0xJZmZGIR",
   "metadata": {
    "id": "0kb0xJZmZGIR"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import models\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import math\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from scipy.stats import  spearmanr\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "\n",
    "def form_data(data_frame, type):\n",
    "  \"\"\"\n",
    "  Input a data frame and return the dataloder.\n",
    "  \"\"\"\n",
    "\n",
    "  for row in range(len(data_frame)):\n",
    "    score = float(data_frame.loc[row, \"score\"])/5  #normalise\n",
    "    inp_ex = InputExample(texts = [data_frame.loc[row, \"sent_a\"], data_frame.loc[row, \"sent_b\"]], label=score)\n",
    "    if type == 'train':\n",
    "        train_samples.append(inp_ex)\n",
    "    if type == 'test':\n",
    "        test_samples.append(inp_ex)\n",
    "    if type == 'dev':\n",
    "        dev_samples.append(inp_ex)\n",
    "    if type == 'train':\n",
    "    return DataLoader(train_samples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def get_model_predicts(train_example, trained_model):\n",
    "  \"\"\"\n",
    "    Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
    "    \"\"\"\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    scores = []\n",
    "\n",
    "    for example in train_example:\n",
    "        sentences1.append(example.texts[0])\n",
    "        sentences2.append(example.texts[1])\n",
    "        scores.append(example.label)\n",
    "\n",
    "\n",
    "    embedding1 = trained_model.encode(sentences1, batch_size=BATCH_SIZE, convert_to_numpy=True)\n",
    "    embedding2 = trained_model.encode(sentences2, batch_size=BATCH_SIZE, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "\n",
    "    cosine_scores = 1 - (paired_cosine_distances(embedding1, embedding2))\n",
    "\n",
    "\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45471096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dataloader_train = form_data(df_train, 'train')\n",
    "form_data(df_test, 'test')\n",
    "form_data(df_dev, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33af0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model intialization\n",
    "\n",
    "#pooling layer creates sentence embeddings from word embeddings by clubbing them\n",
    "layer_pooling = models.Pooling(CONEXTUAL_MODEL_TYPE.get_word_embedding_dimension(), pooling_mode_mean_tokens=True, \n",
    "                               pooling_mode_cls_token=False, pooling_mode_max_tokens=False)\n",
    "\n",
    "#adding a new layer to the output of the output of pooling helps learn task specific features\n",
    "layer_dense = models.Dense(in_features=layer_pooling.get_sentence_embedding_dimension(), out_features=OUT_DIM_DENSE, activation_function=nn.Tanh())\n",
    "\n",
    "model3 = SentenceTransformer(modules=[CONEXTUAL_MODEL_TYPE, layer_pooling, layer_dense])\n",
    "\n",
    "#calculate cosine similarity loss\n",
    "loss = losses.CosineSimilarityLoss(model = model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7024179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 347/347 [00:33<00:00, 10.51it/s]\n",
      "Iteration: 100%|██████████| 347/347 [00:32<00:00, 10.71it/s]\n",
      "Iteration: 100%|██████████| 347/347 [00:35<00:00,  9.84it/s]\n",
      "Epoch: 100%|██████████| 3/3 [01:40<00:00, 33.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model3.fit(train_objectives=[(dataloader_train, loss)], epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1b44f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8212583479361734\n"
     ]
    }
   ],
   "source": [
    "# prediction of spearman score over train sset\n",
    "cosine_scores = get_model_predicts(train_samples, model3)\n",
    "spearman_scores, _ = spearmanr(df_train['score'], cosine_scores)\n",
    "print(spearman_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4cf8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8572441472850028\n"
     ]
    }
   ],
   "source": [
    "# prediction of spearman score over dev sset\n",
    "cosine_scores = get_model_predicts(dev_samples, model3)\n",
    "spearman_scores, _ = spearmanr(df_dev['score'], cosine_scores)\n",
    "print(spearman_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34011796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8182538548138503\n"
     ]
    }
   ],
   "source": [
    "# prediction of spearman score over test sset\n",
    "cosine_scores = get_model_predicts(test_samples, model3)\n",
    "spearman_scores, _ = spearmanr(df_test['score'], cosine_scores)\n",
    "print(spearman_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e48b6ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.auto_model.embeddings.word_embeddings.weight\n",
      "0.auto_model.embeddings.position_embeddings.weight\n",
      "0.auto_model.embeddings.token_type_embeddings.weight\n",
      "0.auto_model.embeddings.LayerNorm.weight\n",
      "0.auto_model.embeddings.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.0.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.0.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.0.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.0.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.0.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.0.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.0.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.0.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.0.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.0.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.0.output.dense.weight\n",
      "0.auto_model.encoder.layer.0.output.dense.bias\n",
      "0.auto_model.encoder.layer.0.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.0.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.1.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.1.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.1.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.1.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.1.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.1.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.1.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.1.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.1.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.1.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.1.output.dense.weight\n",
      "0.auto_model.encoder.layer.1.output.dense.bias\n",
      "0.auto_model.encoder.layer.1.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.1.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.2.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.2.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.2.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.2.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.2.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.2.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.2.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.2.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.2.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.2.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.2.output.dense.weight\n",
      "0.auto_model.encoder.layer.2.output.dense.bias\n",
      "0.auto_model.encoder.layer.2.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.2.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.3.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.3.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.3.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.3.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.3.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.3.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.3.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.3.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.3.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.3.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.3.output.dense.weight\n",
      "0.auto_model.encoder.layer.3.output.dense.bias\n",
      "0.auto_model.encoder.layer.3.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.3.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.4.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.4.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.4.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.4.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.4.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.4.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.4.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.4.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.4.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.4.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.4.output.dense.weight\n",
      "0.auto_model.encoder.layer.4.output.dense.bias\n",
      "0.auto_model.encoder.layer.4.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.4.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "0.auto_model.encoder.layer.5.output.dense.weight\n",
      "0.auto_model.encoder.layer.5.output.dense.bias\n",
      "0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "0.auto_model.pooler.dense.weight\n",
      "0.auto_model.pooler.dense.bias\n",
      "2.linear.weight\n",
      "2.linear.bias\n"
     ]
    }
   ],
   "source": [
    "# for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "#     if idx <= 83:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "\n",
    "# for idx, (name, param) in enumerate(model3.named_parameters()):\n",
    "#     if param.requires_grad == True:\n",
    "#         print(name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
