{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "from nltk.corpus import words\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('A1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 05 14:26:50 2009</td>\n",
       "      <td>About to get threaded and scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thu May 14 10:13:55 2009</td>\n",
       "      <td>@awaisnaseer I like Shezan Mangooo too!!! I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jun 05 21:02:20 2009</td>\n",
       "      <td>worked on my car after work. showering then go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 14 22:25:52 2009</td>\n",
       "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 00:42:12 2009</td>\n",
       "      <td>@gfalcone601 Aww Gi.don't worry.we'll vote for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL                 DATE_TIME  \\\n",
       "0      0  Fri Jun 05 14:26:50 2009   \n",
       "1      1  Thu May 14 10:13:55 2009   \n",
       "2      1  Fri Jun 05 21:02:20 2009   \n",
       "3      1  Sun Jun 14 22:25:52 2009   \n",
       "4      1  Sun May 31 00:42:12 2009   \n",
       "\n",
       "                                                TEXT  \n",
       "0                  About to get threaded and scared   \n",
       "1  @awaisnaseer I like Shezan Mangooo too!!! I ha...  \n",
       "2  worked on my car after work. showering then go...  \n",
       "3  @Marama Actually we start this afternoon!  I w...  \n",
       "4  @gfalcone601 Aww Gi.don't worry.we'll vote for...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL and HTML Tag Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(tokens):\n",
    "    return re.sub(r'http\\S+','', tokens)\n",
    "#print(remove_URL(\"worked on my car after work. showering then going to bed. sooooooooooo tired. sparrow signing out  &lt;Cowboy Up&gt;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@nakulshenoy Lol, that and twiiter killed the blogger are far apart. Btw, what is blogging in the traditional sense may i know? \n"
     ]
    }
   ],
   "source": [
    "def remove_HTMLTag(tokens):\n",
    "    return re.sub(r'&\\w+;','', tokens)\n",
    "print(remove_HTMLTag(\"@nakulshenoy Lol, that and &quot;twiiter killed the blogger&quot; are far apart. Btw, what is &quot;blogging in the traditional sense&quot; may i know? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "     return (word_tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokens(tokens):\n",
    "        # Porter stemmer\n",
    "        ps = PorterStemmer()\n",
    "        stem_tokens = [ps.stem(x) for x in tokens]\n",
    "        \n",
    "        return stem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "        \n",
    "    return lemmatize_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Puntuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_tokens(tokens):\n",
    "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
    "    return tokens_sans_punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces_tokens(tokens):\n",
    "    tokenized = re.findall(r'(\\w+)', tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(tokens):\n",
    "    txt = TextBlob(tokens)\n",
    "    return txt.correct()\n",
    "#print(spelling_correction(\"@Smithycurt NOBBBBBYYY NOBBY NOB ROTTY  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopwords = {'is','s','am','or','who','as','from','him','each','the','themselves','until','below','are','we','these','your','too','to','still','now','with','me','actually','this','i','will','have','by','it','be','a','of','off','on','many','something','just','got','about','go','get','and','his','like','his','some','her','hers','ourselves','but','again','between','yourself','there','once','during','out','very','having','they','own','an','be','for','do','its','it','yours','such','into','who','as','from','him','each','themselves','until','below','are','we','these','your','his','through','nor','me','were','her','more','himself','this','down','should','their','while','above','both','up','ours','she','all','no','after','before','few','how','further','here','than','doing','if','iff','theirs','my','against','whom','over','why','so','can','did','not','does','don','t','myself','been','same','under'}\n",
    "    res  = [word for word in re.split(\"\\W+\",str(tokens)) if word.lower() not in stopwords]\n",
    "    return res\n",
    "#print(remove_stopwords(['just', 'got', 'out', 'the', 'hot', 'tub', 'about', 'to', 'go', 'get', 'a', 'movie', 'and', 'ice', 'cream', 'with', 'mt', 'll', 'his', 'and', 'call', 'it', 'a', 'night', 'nothing', 'like', 'some', 'his', 'time'])\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    text = input(\"Enter your text: \")\n",
    "   \n",
    "    task1 = remove_URL(text)\n",
    "    print(\"\\n Remove URL \\n\")\n",
    "    print(task1)\n",
    "    \n",
    "    task2 = remove_HTMLTag(task1)\n",
    "    print(\"\\n Remove HTML Tag \\n\")\n",
    "    print(task2)\n",
    "    \n",
    "    task3 = spelling_correction(str(task2))\n",
    "    print(\"\\n Spelling Correction \\n\")\n",
    "    print(task3)\n",
    "    \n",
    "    task4 = tokenization(str(task3))\n",
    "    print(\"\\n Tokenization \\n\")\n",
    "    print(task4)\n",
    "    \n",
    "    task5 = remove_punctuation_tokens(task4)\n",
    "    print(\"\\n Remove Puntuations \\n\")\n",
    "    print(task5) \n",
    "   \n",
    "    task6 = remove_stopwords((task5))\n",
    "    print(\"\\n Remove stopwords \\n\")\n",
    "    print(task6)\n",
    "    \n",
    "    task7 = remove_white_spaces_tokens(str(task6))\n",
    "    print(\"\\n Remove extra white spaces \\n\")\n",
    "    print(task7)\n",
    " \n",
    "    task8 = stemming_tokens(task7)\n",
    "    print(\"\\n Stemming \\n\")\n",
    "    print(task8)\n",
    "    \n",
    "    task9 = lemmatize_tokens(task8)\n",
    "    print(\"\\n Lemmatization \\n\")\n",
    "    print(task9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: just got out the hot tub....about to go get a movie and ice cream with mt lil sis and call it a night!!!  nothing like some sis time!!! \n",
      "\n",
      " Remove URL \n",
      "\n",
      "just got out the hot tub....about to go get a movie and ice cream with mt lil sis and call it a night!!!  nothing like some sis time!!! \n",
      "\n",
      " Remove HTML Tag \n",
      "\n",
      "just got out the hot tub....about to go get a movie and ice cream with mt lil sis and call it a night!!!  nothing like some sis time!!! \n",
      "\n",
      " Spelling Correction \n",
      "\n",
      "just got out the hot tub....about to go get a movie and ice cream with mt ll his and call it a night!!!  nothing like some his time!!! \n",
      "\n",
      " Tokenization \n",
      "\n",
      "['just', 'got', 'out', 'the', 'hot', 'tub', '....', 'about', 'to', 'go', 'get', 'a', 'movie', 'and', 'ice', 'cream', 'with', 'mt', 'll', 'his', 'and', 'call', 'it', 'a', 'night', '!', '!', '!', 'nothing', 'like', 'some', 'his', 'time', '!', '!', '!']\n",
      "\n",
      " Remove Puntuations \n",
      "\n",
      "['just', 'got', 'out', 'the', 'hot', 'tub', '', 'about', 'to', 'go', 'get', 'a', 'movie', 'and', 'ice', 'cream', 'with', 'mt', 'll', 'his', 'and', 'call', 'it', 'a', 'night', '', '', '', 'nothing', 'like', 'some', 'his', 'time', '', '', '']\n",
      "\n",
      " Remove stopwords \n",
      "\n",
      "['', 'hot', 'tub', 'movie', 'ice', 'cream', 'mt', 'll', 'call', 'night', 'nothing', 'time', '']\n",
      "\n",
      " Remove extra white spaces \n",
      "\n",
      "['hot', 'tub', 'movie', 'ice', 'cream', 'mt', 'll', 'call', 'night', 'nothing', 'time']\n",
      "\n",
      " Stemming \n",
      "\n",
      "['hot', 'tub', 'movi', 'ice', 'cream', 'mt', 'll', 'call', 'night', 'noth', 'time']\n",
      "\n",
      " Lemmatization \n",
      "\n",
      "['hot', 'tub', 'movi', 'ice', 'cream', 'mt', 'll', 'call', 'night', 'noth', 'time']\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
